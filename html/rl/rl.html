<!doctype html>
<html lang="en">
  <head>
    <!--Latex css-->
    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <!--Code highlighting-->
    <link rel="stylesheet" href="https://latex.now.sh/prism/prism.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <!--Misc-->
    <meta charset="UTF-8">
    <!--Math code-->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!--No cache, to refresh on GitHub pages fast.-->
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
  </head>

  <body id="top">
    <span class="sidenote left">
      <a href="https://asicoderofficial.github.io/asiweb.github.io/">
        <img src="../../logos/logo.png" width="200" height="100">
      </a>
    </span>
    <!-- Header -->
    <header>
      <h1><span class="latex">Reinforcement learning</h1>
      <p class="author">
        What is it?<br>
      </p>
    </header>
    <div class="abstract">
      <h2>Abstract</h2>
      <p>Humans partly learn by observing, trying, failing and retrying in order to achieve their goal. That's the main idea behind reinforcement learning. In the present page, the basic setup and concepts of this type of machine learning are exposed, as well as a brief overview of methods.</p>
    </div>

    <nav role="navigation">
      <!-- Index of contents-->
      <h2>Contents</h2>
      <ol>
        <li>
          <a href="#introduction">Introduction</a>
        </li>
        <li>
          <a href="#learningenvironment">Components and workflow</a>
        </li>
        <li>
          <a href="#notation">Notation</a>
        </li>
        <li>
          <a href="#behindthescenes">Behind the scenes</a>
        </li>
        <li>
          <a href="#resources">Useful resources</a>
        </li>
      </ol>
    </nav>
    <main>
        <article>
            <div id="introduction">
                <h2>1. Introduction</h2>
                <p>Reinforcement learning is a branch of machine learning, that deals with how to learn control strategies to optimally interact with an environment from experience.</p>
                <p>This biologically inspired idea, sets an interactive framework that then proposes an optimization problem, both of which we'll inspect next.</p>
            </div>
            <div id="learningenvironment">
                <h2>2. Components and workflow</h2>
                <figure>
                    <img id="rl-diagram" src="rl-diagram.png" width="600" height="400">
                    <figcaption>
                        Figure 1.1: Reinforcement learning workflow. Source: <a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da">Towards Data Science</a>.
                    </figcaption>
                </figure>
                <p></p>
                <p>
                As seen in Figure <a href="#rl-diagram">1.1</a>, the workflow is as follows:
                <ol>
                    <li>At a given time <i>t</i>, the <i>Agent</i> is observing the <i>State</i> for that time step.</li>
                    <li>The <b>Agent</b> interacts with the <b>Environment</b> by carrying out an <b>action</b> <i>A</i> at time step <i>t</i>.</li>
                    <li>The <i>Action</i>, may or may not change the <i>Environment</i>. Depending on its consequences, the <i>Agent</i> will get a concrete reward, and the state that it can observe, will transition to the one corresponding to the next time step.</li>
                    <li>If no termination state is reached, the process continues from 1. Else, it ends.</li>
                </ol>
                </p>
                <p>
                Now that we've seen the workflow, let's define more in depth each of the components.
                </p>
                <h3>Agent</h3>
                <p>The agent is the entity that interacts with the environment. It is the one that carries out the actions, and observes the state of the environment. It is also the one that learns from the environment, and tries to optimize its actions.</p>
                <p>There may be one or more agents in the environment.</p>
                <h3>Environment</h3>
                <p>The environment is the entity that the agent interacts with. It is the one that changes its state depending on the actions of the agent, and gives rewards to the agent.</p>
                <h3>State</h3>
                <p>The state is the representation of the environment at a given time step. It is the one that the agent observes at each time step separately, and that the environment changes depending on the actions of the agent.</p>
                <p>Each state can be either:
                    <ul>
                        <li>Fully observable: The agent can observe all the information about the state.</li>
                        <li>Partially observable: The agent can only observe a subset of the information about the state.</li>
                    </ul>
                </p>
                <h3>Action</h3>
                <p>The action is the entity that the agent carries out at each time step. It is the one that changes the state of the environment, and that the environment rewards or punishes the agent for.</p>
                <h3>Reward</h3>
                <p>The reward is the entity that the environment gives to the agent at each time step. It is the one that the agent tries to maximize, and that the agent learns from.</p>
                <p>
                The reward can be either:
                    <ul>
                        <li>Immediate: The reward is given at the same time step that the action is carried out, which would be a fully supervised learning approach, except for the fact that there is no actual dataset with target labels for each sample before beginning to solve the optimization problem.</li>
                        <li>Delayed: The reward is given at a later time step. In this case, we would categorize the learning method as semi-supervised.</li>
                    </ul>
                </p>
                <p></p>
            </div>
            <div id="notation">
                <h2>3. Notation</h2>
            </div>
            <div id="behindthescenes">
                <h2>4. Behind the scenes</h2>
                <p>
                  The goal of reinforcement learning is to <b>design an optimal probabilistic policy function 
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mstyle displaystyle="true">
                        <mi>&#x3C0;</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>s</mi>
                          <mo>,</mo>
                          <mi>a</mi>
                          <mo>)</mo>
                        </mrow>
                      </mstyle>
                    </math>
                  for deciding which actions to take given a state to maximize the future reward</b>.</p>
                  <p>
                  That means we aim to generate the most optimal sequence (denominated as <i>trajectory</i> or <i>episode</i>) of time contiguous [state, action, reward] tuples of the form:
                  <div align="center">
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mstyle displaystyle="true">
                        <mi>&#x3C4;</mi>
                        <mo>=</mo>
                        <mrow>
                          <mo>(</mo>
                          <msub>
                            <mi>S</mi>
                            <mn>0</mn>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>A</mi>
                            <mn>0</mn>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>R</mi>
                            <mn>0</mn>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>S</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>A</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>R</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>,</mo>
                          <mo>...</mo>
                          <mo>,</mo>
                          <msub>
                            <mi>S</mi>
                            <mrow>
                              <mi>t</mi>
                              <mo>-</mo>
                              <mn>1</mn>
                            </mrow>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>A</mi>
                            <mrow>
                              <mi>t</mi>
                              <mo>-</mo>
                              <mn>1</mn>
                            </mrow>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>R</mi>
                            <mrow>
                              <mi>t</mi>
                              <mo>-</mo>
                              <mn>1</mn>
                            </mrow>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>S</mi>
                            <mi>t</mi>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>A</mi>
                            <mi>t</mi>
                          </msub>
                          <mo>,</mo>
                          <msub>
                            <mi>R</mi>
                            <mi>t</mi>
                          </msub>
                          <mo>)</mo>
                        </mrow>
                      </mstyle>
                    </math>
                  </div>

                </p>
                <p>
                  To start, the state 
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mstyle displaystyle="true">
                      <msub>
                        <mi>S</mi>
                        <mn>0</mn>
                      </msub>
                    </mstyle>
                  </math>
                  is randomly sampled from the <i>start-state distribution</i>,
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mstyle displaystyle="true">
                      <msub>
                        <mi>&#x3C1;</mi>
                        <mn>0</mn>
                      </msub>
                    </mstyle>
                  </math>
                  , in which 
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mstyle displaystyle="true">
                      <msub>
                        <mi>S</mi>
                        <mn>0</mn>
                      </msub>
                      <mo>~</mo>
                      <msub>
                        <mi>&#x3C1;</mi>
                        <mn>0</mn>
                      </msub>
                      <mrow>
                        <mo>(</mo>
                        <mo>&#x22C5;</mo>
                        <mo>)</mo>
                      </mrow>
                    </mstyle>
                  </math>
                .
                </p>
                <p>
                  Transitions between states may be:
                  <ul>
                    <li>Deterministic: for the same state
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mstyle displaystyle="true">
                          <msub>
                            <mi>S</mi>
                            <mn>t</mn>
                          </msub>
                        </mstyle>
                      </math>
                    and action
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mstyle displaystyle="true">
                          <msub>
                            <mi>A</mi>
                            <mn>t</mn>
                          </msub>
                        </mstyle>
                      </math>
                    , the resulting state 
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mstyle displaystyle="true">
                          <msub>
                            <mi>S</mi>
                            <mrow>
                              <mi>t</mi>
                              <mo>+</mo>
                              <mn>1</mn>
                            </mrow>
                          </msub>
                        </mstyle>
                      </math>
                    obtained from the computation of the function
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mstyle displaystyle="true">
                        <mrow>
                          <mi>f</mi>
                          <mrow>
                            <mo>(</mo>
                            <msub>
                              <mi>S</mi>
                              <mi>t</mi>
                            </msub>
                            <mo>,</mo>
                            <msub>
                              <mi>A</mi>
                              <mi>t</mi>
                            </msub>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </mstyle>
                    </math>
                    , never varies.
                    </li>
                    <li>Stochastic: instead of a function, the next state is sampled from a probability distribution.
                      <div align="center">
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                          <mstyle displaystyle="true">
                            <msub>
                              <mi>S</mi>
                              <mrow>
                                <mi>t</mi>
                                <mo>+</mo>
                                <mn>1</mn>
                              </mrow>
                            </msub>
                            <mo>~</mo>
                            <mi>&#x3C1;</mi>
                            <mrow>
                              <mo>(</mo>
                              <msub>
                                <mi>S</mi>
                                <mrow>
                                  <mi>t</mi>
                                  <mo>+</mo>
                                  <mn>1</mn>
                                </mrow>
                              </msub>
                              <mrow>
                                <mo>&#x2223;</mo>
                              </mrow>
                              <msub>
                                <mi>S</mi>
                                <mi>t</mi>
                              </msub>
                              <mo>,</mo>
                              <msub>
                                <mi>A</mi>
                                <mi>t</mi>
                              </msub>
                              <mo>)</mo>
                            </mrow>
                          </mstyle>
                        </math>
                      </div>
                    </li>
                  </ul> 
                </p>
                <p>
                  The mentioned trajectories, are possible because the agent is describing a <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Process</a> (see Figure <a href="#mdpexample">4.1</a>), which itself is an extension of <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>.
                  It exhibits the Markov (<i>memoryless</i>) property, as probabilities of different outcomes are not dependent on the past, just the current state.
                  In order to find the optimal trajectory, we can follow 
                <figure>
                    <img id="mdpexample" src="mdpexample.png" width="600" height="400">
                    <figcaption>
                        Figure 4.1: Example of a Markov Decision Process. Source: <a href="https://towardsdatascience.com/understanding-the-markov-decision-process-mdp-8f838510f150">Towards Data Science</a>.
                    </figcaption>
                </figure>

                </p>
                <p>
                  For achieving the previously mentioned goal, there are two different strategies.
                </p>
                <p>
                  <h3>Value learning</h3>
                  Tries to quantify the <b>value</b> (expected return) of each state-action pair which can be obtained following a policy
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mstyle displaystyle="true">
                        <mi>&#x3C0;</mi>
                      </mstyle>
                    </math>
                    :
                    <p>
                    <div align="center">
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mstyle displaystyle="true">
                        <msub>
                          <mi>V</mi>
                          <mi>&#x3C0;</mi>
                        </msub>
                        <mrow>
                          <mo>(</mo>
                          <mi>s</mi>
                          <mo>)</mo>
                        </mrow>
                        <mo>=</mo>
                        <msub>
                          <mi>&#x1D53C;</mi>
                          <mi>&#x3C0;</mi>
                        </msub>
                        <mrow>
                          <mo>{</mo>
                          <msub>
                            <mi>R</mi>
                            <mi>t</mi>
                          </msub>
                          <mrow>
                            <mo>&#x2223;</mo>
                          </mrow>
                          <msub>
                            <mi>s</mi>
                            <mn>0</mn>
                          </msub>
                          <mo>=</mo>
                          <mi>s</mi>
                          <mo>}</mo>
                        </mrow>
                      </mstyle>
                    </math>
                    </div>
                    <div align="center">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mstyle displaystyle="true">
                          <mo>=</mo>
                          <msub>
                            <mi>&#x1D53C;</mi>
                            <mi>&#x3C0;</mi>
                          </msub>
                          <mrow>
                            <mo>{</mo>
                            <munder>
                              <mo>&#x2211;</mo>
                              <mi>t</mi>
                            </munder>
                            <msup>
                              <mi>&#x3B3;</mi>
                              <mi>t</mi>
                            </msup>
                            <msub>
                              <mi>r</mi>
                              <mi>t</mi>
                            </msub>
                            <mrow>
                              <mo>&#x2223;</mo>
                            </mrow>
                            <msub>
                              <mi>s</mi>
                              <mn>0</mn>
                            </msub>
                            <mo>=</mo>
                            <mi>s</mi>
                            <mo>}</mo>
                          </mrow>
                        </mstyle>
                      </math>
                    </div>
                    <div align="center">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mstyle displaystyle="true">
                          <mo>=</mo>
                          <msub>
                            <mi>&#x1D53C;</mi>
                            <mi>&#x3C0;</mi>
                          </msub>
                          <mrow>
                            <mo>{</mo>
                            <msub>
                              <mi>r</mi>
                              <mn>0</mn>
                            </msub>
                            <mo>+</mo>
                            <mrow>
                              <munderover>
                                <mo>&#x2211;</mo>
                                <mrow>
                                  <mi>t</mi>
                                  <mo>=</mo>
                                  <mn>1</mn>
                                </mrow>
                                <mo>&#x221E;</mo>
                              </munderover>
                            </mrow>
                            <msup>
                              <mi>&#x3B3;</mi>
                              <mi>t</mi>
                            </msup>
                            <msub>
                              <mi>r</mi>
                              <mi>t</mi>
                            </msub>
                            <mrow>
                              <mo>&#x2223;</mo>
                            </mrow>
                            <msub>
                              <mi>s</mi>
                              <mn>1</mn>
                            </msub>
                            <mo>=</mo>
                            <msup>
                              <mi>s</mi>
                              <mo>&#x2032;</mo>
                            </msup>
                            <mo>}</mo>
                          </mrow>
                        </mstyle>
                      </math>
                    </div>
                    <div align="center">
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mstyle displaystyle="true">
                          <mo>=</mo>
                          <msub>
                            <mi>&#x1D53C;</mi>
                            <mi>&#x3C0;</mi>
                          </msub>
                          <mrow>
                            <mo>{</mo>
                            <msub>
                              <mi>r</mi>
                              <mn>0</mn>
                            </msub>
                            <mo>+</mo>
                            <mi>&#x3B3;</mi>
                            <mi>V</mi>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi>s</mi>
                                <mo>&#x2032;</mo>
                              </msup>
                              <mo>)</mo>
                            </mrow>
                            <mo>}</mo>
                          </mrow>
                        </mstyle>
                      </math>

                    </div>
                    </p>
                for every time step 
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <mstyle displaystyle="true">
                    <mi>t</mi>
                  </mstyle>
                </math>
                .
                </p>
                <p>
                  When maximized over all possible policies, it conforms the <b>Bellman Equation</b>,
                  <div align="center">
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mstyle displaystyle="true">
                        <mi>V</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>s</mi>
                          <mo>)</mo>
                        </mrow>
                        <mo>=</mo>
                        <munder>
                          <mo>max</mo>
                          <mi>&#x3C0;</mi>
                        </munder>
                        <mi>E</mi>
                        <mrow>
                          <mo>{</mo>
                          <msub>
                            <mi>r</mi>
                            <mn>0</mn>
                          </msub>
                          <mo>+</mo>
                          <mi>&#x3B3;</mi>
                          <mi>V</mi>
                          <mrow>
                            <mo>(</mo>
                            <msup>
                              <mi>s</mi>
                              <mo>&#x2032;</mo>
                            </msup>
                            <mo>)</mo>
                          </mrow>
                          <mo>}</mo>
                        </mrow>
                      </mstyle>
                    </math>
                  </div>
                  which if satisfied, means that the problems it is being applied to exhibits an optimal substructure, hence, it can be solved by using dynamic programming.
                </p>
                <p>
                  It is important to note that 
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mstyle displaystyle="true">
                      <mi>&#x3B3;</mi>
                      <mspace width="10px"></mspace>
                      <mi>&#x3F5;</mi>
                      <mrow>
                        <mo>[</mo>
                        <mn>0</mn>
                        <mo>,</mo>
                        <mn>1</mn>
                        <mo>]</mo>
                      </mrow>
                    </mstyle>
                  </math>
                  represents a constant, called the <i>discount rate</i>, which makes more worthy the rewards obtained during the first actions.
                </p>
                <p>
                  From a human learning point of view, this methodology is closer to how a baby learns new concepts, by exploring and experimenting, not trying to find a previously known optimal path to the solution.
                </p>
                <p>
                  <h3>Policy learning</h3>
                </p>
            </div>
        </article>
    </main>