<!doctype html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <!--Code highlighting-->
    <link rel="stylesheet" href="../../../../code-highlighting/prism.css">
    <script src="../../../../code-highlighting/prism.js"></script>
    <!--Math code-->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <span class="sidenote left">
    <a href="https://asicoderofficial.github.io/asiweb.github.io/">
      <img src="../../../../../../../logos/logo.png" width="200" height="100">
    </a>
  </span>
  <header>
    <h1><span class="latex">Discrete mountain car</h1>
    <p class="author">
      Reach the flag as fast as possible, using a discrete set of actions.<br>
    </p>
  </header>
  <body id="top">
    <div class="abstract">
      <h2>Abstract</h2>
      <p>FILL ABSTRACT AT THE END</p>
    </div>
    <nav class="toc" role="navigation">
      <h2>Contents</h2>
        <ol>
          <li><a href="#introduction">Introduction</a></li>
          <li><a href="#environment">Environment</a></li>
          <li><a href="#basic-setup">Basic setup</a></li>
          <li><a href="#naive-strategies">Naive strategies</a></li>
            <ol>
              <li><a href="#maximize-mean-reward">Maximize mean reward</a></li>
              <li><a href="#sin">Sin function rewarding</a></li>
            </ol>
          <li><a href="#conclusion">Conclusion</a></li>
        </ol>
    </nav>
    <main>
      <article>
        <div id="introduction">
          <h2>1. Introduction</h2>
          <p>The <a href="https://www.gymlibrary.dev/environments/classic_control/mountain_car/" target="_blank">discrete mountain car</a> is a deterministic environment,
          where a car is initially placed at the bottom of a sinusoidal valley, and its goal is to reach the flag at the top, as quickly as possible.</p>
        </div>
        <div id="environment">
          <h2 id="environment">2. Environment</h2>
          <figure>
            <img src="media/circuit_overview.png" width="600" height="400">
            <figcaption>
              Initial position of the car, and the flag at the top.
            </figcaption>
          </figure>
          <p></p>
          Each state is defined by:
          <ul>
            <li>Position of the car along the x-axis.</li>
            <li>Velocity of the car.</li>
          </ul>
          At each time step, the agent can take 3 actions:
          <ul>
            <li>Accelerate to the left.</li>
            <li>Do nothing.</li>
            <li>Accelerate to the right.</li>
          </ul>
          Initially, the car is placed at a random point in the x-axis between [-0.6, -0.4], and its velocity is 0.
        </div>
        <div id="basic-setup">
          <h2 id="basic-setup">3. Basic setup</h2>
          <p>Import and build the environment.</p>
          <pre>
            <code class="language-python">
env_id = 'MountainCar-v0'
env = gym.make(env_id)</code>
          </pre>
          Here are the most important basic functions to call to the environment:
          <pre>
            <code class="language-python">
# Reset the environment: sets the car's initial position and velocity.
env.reset()
# Render the environment to see it.
env.render()
# Perform an action: accelerate to the left, right, or nothing.
env.step(action)
# Sleep: wait a little bit between actions, to see the car move properly.
time.sleep(0.01)</code>
          </pre>
        </div>
        <div id="naive-strategies">
          <h2 id="naive-strategies">4. Naive strategies</h2>
          <p>In this first chapter, I will implement a few simple strategies that require no theory to be coded, just some simple reasoning.</p>
          <p>Thereupon, they are generally not sophisticated to achieve a good performance, but they are a good starting point to understand the environment and create a baseline.</p>
          <p>The code can be found <a href="https://github.com/asicoderOfficial/RL-implementations/blob/naive/src/discrete_mountain_car/naive.py">here</a>. Each implementation explained at the following subsections, has its own class, with a parent class <code>MCDNaive</code>.</p>
        </div>
        <div id="maximize-mean-reward">
          <h2 id="maximize-mean-reward">4.1 Maximize mean reward</h2>
          <p>
          The simplest form of reward function is the mean of rewards generated by the environment.
          </p>
          <p>
          After around 700 steps, the car overfits learning to gain momentum (Figure <a href="#naivecarmeangif">4.1</a>), but it does not take advantage of it, so it ends up only moving backwards to gain as much momentum as possible.
          </p>
          <figure>
            <img src="media/naivecar_mean.gif" width="600" height="400" id="naivecarmeangif">
            <figcaption>
              <p style="text-align:center">
                <b>Figure 4.1</b>
              </p>
            </figcaption>
          </figure>
          <p>
            See the source at child class <code>MCDNaiveMean</code>.
          </p>
        </div>
        <div>
          <h2 id="sin">4.2 Sin function rewarding</h2>
          <p>
            As we know that the hill is a sin function, we can get the corresponding y-axis, also, we know that the higher the car is,
            either the more momentum it is obtaining or the closer it is to the flag.
          </p>
          <p>
            Following that reasoning, we define the reward function as:
          <pre>
            <code class="language-python">
(rewards[curr_state][action] + # Previous reward for selected action.
(gym_reward * \ # Gym reward, penalizing as it is always -1.
(1 / (sin_of_pos * sin_reward_reducing)) * \ # y-axis value ('height').
(1 / (abs(velocity) * velocity_reward_reducing)) * \ # current velocity.
(1 / (abs(env_high - pos) * flag_distance_mult)))) \  # distance to flag.
/ 2 # Average.</code>
          </pre>
          Where 4 new variables are defined:
          <ul>
            <li><code>sin_of_pos = sin(pos)</code></li>
            <li><code>sin_reward_reducing</code>: to control the weight of the y-axis value. Inversely proportional.</li>
            <li><code>velocity_reward_reducing</code>: to control the weight of the velocity. Inversely proportional.</li>
            <li><code>flag_distance_mult</code>: to control the weight of the distance to the flag. Directly proportional.</li>
          </ul>
          </p>
          <p>
            As seen in Figure <a href="#naivecarsingif">4.2</a>, the car, starting from -0.4177 is able to reach the flag in 2075 steps, 
            taking 54 seconds to do so. Thus, it will be used as baseline.
          </p>
          <figure>
            <img src="media/sin.gif" width="600" height="400" id="naivecarsingif">
              <p style="text-align:center">
                <b>Figure 4.2</b>
              </p>
          </figure>
          <p>
            See the source at child class <code>MCDNaiveSin</code>.
          </p>
        </div>
        <div id="conclusion">
          <h2 id="conclusion">5. Conclusion</h2>
        </div>

      </article>
    </main>
  </body>
  <h1></h1>
</html>
