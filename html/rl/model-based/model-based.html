<!doctype html>
<html lang="en">
  <head>
    <!--Latex css-->
    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <!--Code highlighting-->
    <link rel="stylesheet" href="https://latex.now.sh/prism/prism.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <!--Math code-->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!--Misc-->
    <meta charset="UTF-8">
    <title>Asier Serrano personal webpage & blog.</title>
    <meta name="description" content="Open source, maths & more.">
    <meta name="keywords" content="open source, reinforcement learning, maths, qlearning, openai gym">
    <meta property="og:title" content="Asier Serrano">
    <meta property="og:description" content="A blog & website to explain more in depth my process of thinking and coming up with solutions that are then open sourced on GitHub. Specially focused on reinforcement learning by now.">
    <meta property="og:type" content="website">
    <!--No cache, to refresh on GitHub pages fast.-->
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
  </head>

  <body id="top">
    <span class="sidenote left">
      <a href="https://asicoderofficial.github.io/asiweb.github.io/">
        <img src="../../../logos/logo.png" width="200" height="100">
      </a>
    </span>
    <!-- Header -->
    <header>
      <h1><span class="latex">Model based RL</h1>
      <p class="author">
        When we know the transition probabilities beforehand.<br>
      </p>
    </header>
    <div class="abstract">
      <h2>Abstract</h2>
      <p>
        In the present post, model based reinforcement learning is defined, as well as the existing methods that can be used to solve it.
      </p>
    </div>

    <nav role="navigation">
      <!-- Index of contents-->
      <h2>Contents</h2>
      <ol>
        <li>
          <a href="#introduction">Introduction</a>
        </li>
        <li>
          <a href="#valueiteration">Value & policy iteration</a>
        </li>
        <li>
          <a href="#resources">Useful resources</a>
        </li>
      </ol>
    </nav>
    <main>
        <article>
          <div id="introduction">
            <h2>1. Introduction</h2>
            <p>
              If we know all the probabilities of applying each possible action to each state, and thus the corresponding transitions, the problem we have to solve is how to search in an already known tree of states and actions the optimal action for each state.
            </p>
          </div>
          <div id="valueiteration">
            <h2>2. Value & policy iteration</h2>
            <p>
              See more details <a href="https://asicoderofficial.github.io/asiweb.github.io/rl/model-based/value-iteration.html">here</a>.
            </p>
          </div>
          <div id="resources">
            <h2>3. Useful resources</h2>
              <ul>
                <li>
                  <a href="https://www.youtube.com/watch?v=sJIFUTITfBc&t=179s">Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming</a>, by Steve Brunton.
                </li>
              </ul>
          </div>
        </article>
    </main>
  </body>
</html>
